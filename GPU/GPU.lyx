#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage {url}
\usepackage [numbers]{natbib}
\date{}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 2cm
\headsep 2cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip bigskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Contemporary Computer Science - GPU, Many-core, and Cluster Computing
\end_layout

\begin_layout Author
Tom Robson - hzwr87
\end_layout

\begin_layout Section*
Step 1 - will need to rewrite much of this
\end_layout

\begin_layout Standard
Before attempts are made to improve the performance of the karman code,
 we must examine its current characteristics.
 This has been done using the Intel VTune performance analysis tool, with
 
\begin_inset Formula $t=0.1$
\end_inset

 in the main while loop for reasonable execution time.
 The types of analysis used for this project were General Exploration, Advanced
 Hotspots and HPC Performance Characterisation.
 
\end_layout

\begin_layout Standard
From running these analysis methods, it has been determined that the key
 hotspots are in the functions computeP and setPressureBoundaryConditions.
 These functions are linked, as computeP calls setPressureBoundaryConditions,
 giving us a clear idea that the main issues for this algorithm lie in its
 implementation of pressure.
 From a closer examination of the computeP function, we can see that the
 main bottleneck in this function comes from line 668, given here:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

p[getCellIndex(ix,iy,iz)] += -omega * residual / 6.0 * getH() * getH();
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This line is so computationally expensive because it performs complex mathematic
al functions at floating point precision, meaning that it has a high CPI
 rate.
 The General Exploration analysis states that the primary bottleneck in
 this line comes from the Back-End of the pipeline, with the majority of
 this being as a result of a memory bound, where the Back-End cannot accept
 new operations due to existing operations still running.
 This function also has a substantial core bound, meaning that some of the
 execution ports have become saturated.
 The results of the Advanced Hotspots analysis reveal that this line also
 consumes a large proportion of the CPU time.
 The HPC analysis supports the General Exploration in the fact that there
 is a substantial memory bound present in the whole computeP function, and
 this line in particular.
 
\end_layout

\begin_layout Standard
When examining setPressureBoundaryConditions more closely, the General Explorati
on analysis tells us that this function is limited by the Front-End of the
 pipeline.
 The majority of this is caused by Front-End bandwidth, meaning that not
 all the slots in the pipeline are filled, rather than Front-End latency,
 where none of the slots are filled.
 
\end_layout

\begin_layout Standard
To address these issues, the performance model that should be applied is
 Strong Scaling.
 To calculate the possible speedup due to parallelisation over 
\begin_inset Formula $p$
\end_inset

 processors, was must apply the equation 
\begin_inset Formula $S(p)=\frac{t(1)}{t(p)}$
\end_inset

, where 
\begin_inset Formula $t(1)$
\end_inset

 is the time taken on one processor, and the 
\begin_inset Formula $t(p)$
\end_inset

 is the time taken over p processors.
\begin_inset Formula $t(p)$
\end_inset

 can be calculated using Amdahl's law, 
\begin_inset Formula $t(p)=f.t(1)+\frac{(1-f).t(1)}{p}$
\end_inset

, where 
\begin_inset Formula $f$
\end_inset

 is the fraction of the code that is not parallelisable.
 As computeP is the main hotspot for this code, this must be the target
 of the parallelisation.
 Therefore, the non parallelisable part of the code is everything but computeP.
 These to formulae can be combined and simplified to give 
\begin_inset Formula $S(p)=\frac{1}{f+\frac{(1-f)}{p}}$
\end_inset

.
 To calculate the maximum possible speedup, we will assume 
\begin_inset Formula $p$
\end_inset

 tends to infinity.
 The code was run on Hamiton with parameters 10 0 1600, and computeP consumes
 66.6% of the runtime of the code.
 Elapsed Time: 1588.834s
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Step 2
\end_layout

\begin_layout Standard
To break up the computational domain into blocks, or subproblems, we must
 first determine a suitable block size, set by a global variable.
 To facilitate the process of computation, it has been decided to restrict
 the grid dimensions to be multiples of this block size, supplied by the
 first command line argument.
 If this is not supplied correctly, the system will error.
 To allocate cells to blocks, we loop through the cells that would be in
 the top left corner of each block, and for all cells in the block, check
 if the obstacle is present in any of these cells.
 These blocks are enumerated by a counter, and if a block contains the obstacle,
 this is stored in a separate array of booleans, with the index in the array
 indicating which block the boolean refers to.
 This initialisation process is performed in setupScenario.
\end_layout

\begin_layout Standard
The benefits of this process can be realised in computeP, the main hotspot
 of this code, to improve its run-time.
 We loop through each block, and if the boolean array tells us that this
 block doesn't contain the obstacle, we can vectorise the loops for the
 cells in this block using Intel's SIMD pragmas.
 If one or more of the cells of the block does contain part of the obstacle,
 then the computation must be done in the same way as before, by looping
 through all of the cells individually, and checking if they have a part
 of the obstacle in them, and only performing the update to p if it is not.
 Due to the small size of the obstacle, the majority of the blocks can have
 SIMD vectorisation applied to them, so this vectorisation results in a
 substantial increase in the time taken to run the simulation.
 This process is necessary as the if statement to determine whether a cell
 contains the obstacle presents a barrier to vectorisation.
\end_layout

\begin_layout Standard
The vectorisation report, generated by the command -qopt-report-phase=vec,loop
 -qopt-report=5, tells us that the inner most loop of computeP that we told
 the compiler to vectorise with the SIMD pragma has indeed been vectorised
 successfully, and the vast majority of the rest of the code cannot be vectorise
d.
 
\end_layout

\begin_layout Standard
The effect of this has been measured below, running the code on hamilton
 over 100 timesteps with the grid size parameter varying between 8 and 32.
 As we can see, step 2 represents a significant speedup over step 1.
 The execution of step 2 is approximately 2.4 times faster than step 1.
 Changing the grid size also clearly has a large impact on the runtime.
 Changing the Reynolds number has an effect as well, but this only affects
 number of timesteps, and this has been fixed to 100 for the purposes of
 this experiment.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename Step2.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Comparison of Steps 1 and 2
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Step 3
\end_layout

\begin_layout Standard
To eliminate the data races and facilitate parallelisation, we must implement
 halo layers for the blocks.
 In order to achieve this, we calculate the increase in number of cells
 in each dimension, by adding 2 cells per block in each dimension, and increase
 the size of p to accommodate this.
 In computeP, we loop through the blocks in a similar fashion to step 2,
 and loop through the cells containing data, ignoring the halos.
 The the end of each computeP iteration, the halos are updated using the
 updateHalo function so that they contain the correct neighbour data to
 be used for the calculations in the next iteration.
 
\end_layout

\begin_layout Standard
As halos are not necessary in any of the other arrays, and to facilitate
 accessing p from any of the functions other than computeP, we must implement
 some additional getCellIndex functions.
 When we access p from any other function, the loops have not been expanded
 to include halos, so we must map the value given by the loops to the newly
 expanded array.
 This is done by the function getCellIndexFromOriginals.
 We also need the inverse of this to access the other arrays from the loops
 intended for halos, done by the function getCellIndexFromHalos.
 The final function is for accessing the correct value in p with the halos
 embedded, used in the computeP loops, done using the getCellIndexHalo function.
 
\end_layout

\begin_layout Standard
When these functions have been implemented, we can then move on to the paralleli
sation.
 The OpenMP pragmas will be used to accomplish this, #pragma omp parallel
 for to be specific.
 As the global residual value is appended to in each section, and these
 sections must be brought together.
 This is done using the reduction option from OpenMP, following the pragma
 with the command 'reduction (+:globalResidual)'.
 The SIMD pragma from step 2 has also been changed to the OpenMP equivalent,
 and also uses the reduction for the global residuals.
 
\end_layout

\begin_layout Standard
When this new implementation is run, it appears to not be as fast as step
 2.
 Tests have been run in the same way as in step 2, by varying the grid size
 over 100 timesteps.
 
\end_layout

\begin_layout Standard
Experimentation has also been done with the scheduling options for the paralleli
sation.
 This was done by including the statement schedule(runtime) in the for loop
 pragma, and changing the OMP_SCHEDULE environment variable between static,
 dynamic and guided.
 The best runtime was achieved using the guided scheduling method.
 
\end_layout

\begin_layout Standard
Try changing between options for simd pragma - test is running, second one
 is with intel, first is with omp
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Analysis of step 3
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_body
\end_document
