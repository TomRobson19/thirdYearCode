#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage {url}
\usepackage [numbers]{natbib}
\date{}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 2cm
\headsep 2cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip bigskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Contemporary Computer Science - GPU, Many-core, and Cluster Computing
\end_layout

\begin_layout Author
Tom Robson - hzwr87
\end_layout

\begin_layout Section*
Step 1 - will need to rewrite much of this
\end_layout

\begin_layout Standard
Before attempts are made to improve the performance of the karman code,
 we must examine its current characteristics.
 This has been done using the Intel VTune performance analysis tool on this
 code, with 
\begin_inset Formula $t=0.1$
\end_inset

 for reasonable execution time.
 The types of analysis used for this project were General Exploration, Advanced
 Hotspots and HPC Performance Characterisation.
 
\end_layout

\begin_layout Standard
From running these analysis methods, it has been determined that the key
 hotspots are in the functions computeP and setPressureBoundaryConditions.
 These functions are linked, as computeP calls setPressureBoundaryConditions,
 giving us a clear idea that the main issues for this algorithm lie in its
 implementation of pressure.
 From a closer examination of the computeP function, we can see that the
 main bottleneck in this function comes from line 668, given here:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

p[getCellIndex(ix,iy,iz)] += -omega * residual / 6.0 * getH() * getH();
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This line is so computationally expensive because it performs complex mathematic
al functions and has multiple function calls, meaning tat it has a high
 CPI rate.
 The General Exploration analysis states that the primary bottleneck in
 this line comes from the Back-End of the pipeline, with the majority of
 this being as a result of a memory bound, where the Back-End cannot accept
 new operations due to existing operations still running.
 This function also has a substantial core bound, meaning that some of the
 execution ports have become saturated.
 The results of the Advanced Hotspots analysis reveal that this line also
 consumes a large proportion of the CPU time.
 The HPC analysis supports the General Exploration in the fact that there
 is a substantial memory bound present in the whole computeP function, and
 this line in particular.
 
\end_layout

\begin_layout Standard
When examining setPressureBoundaryConditions more closely, the General Explorati
on analysis tells us that this function is limited by the Front-End of the
 pipeline.
 The majority of this is caused by Front-End bandwidth, meaning that not
 all the slots in the pipeline are filled, rather than Front-End latency,
 where none of the slots are filled.
 
\end_layout

\begin_layout Standard
To address these issues, the performance model that should be applied is
 Strong Scaling.
 To calculate the possible speedup due to parallelisation over 
\begin_inset Formula $p$
\end_inset

 processors, was must apply the equation 
\begin_inset Formula $S(p)=\frac{t(1)}{t(p)}$
\end_inset

, where 
\begin_inset Formula $t(1)$
\end_inset

 is the time taken on one processor, and the 
\begin_inset Formula $t(p)$
\end_inset

 is the time taken over p processors.
\begin_inset Formula $t(p)$
\end_inset

 can be calculated using Amdahl's law, 
\begin_inset Formula $t(p)=f.t(1)+\frac{(1-f).t(1)}{p}$
\end_inset

, where 
\begin_inset Formula $f$
\end_inset

 is the fraction of the code that is not parallelisable.
 As computeP is the main hotspot for this code, this must be the target
 of the parallelisation.
 Therefore, the non parallelisable part of the code is everything but computeP.
 These to formulae can be combined and simplified to give 
\begin_inset Formula $S(p)=\frac{1}{f+\frac{(1-f)}{p}}$
\end_inset

.
 To calculate the maximum possible speedup, we will assume 
\begin_inset Formula $p$
\end_inset

 tends to infinity.
 From the Advanced Hotspot analysis, we can see that 75.9% of the CPU time
 is consumed by computeP, so the value of f needed for this equation is
 0.241.
 Using this value, with p tending to infinity, gives a maximum speedup value
 of 4.149.
 
\end_layout

\begin_layout Standard
################################################################################
################################################################################
#####
\end_layout

\begin_layout Standard

\series bold
Ben got max speedup to be around 6.25
\end_layout

\begin_layout Standard

\series bold
use Amdahls law to get t(p), then calculate speed-up
\end_layout

\begin_layout Standard

\series bold
work this out as if p tend to infinity to get max possible speed-up - will
 become limited by f 
\end_layout

\begin_layout Standard

\series bold
S(p) = 1 /(f + (1 âˆ’ f )/p) - use this as don't need a t(1) figure
\end_layout

\begin_layout Standard
If you're only gonna parallelize computeP then f is everything else, do
 f as percentage/100
\end_layout

\begin_layout Standard
Setpressure is called from computeP and cellindex is basically inlined and
 contributes very little
\end_layout

\begin_layout Section*
Step 2
\end_layout

\begin_layout Standard
Split code across multiple cores (vectorise)
\end_layout

\begin_layout Standard
Split into blocks (maybe 1 for each core?) and determine which code doesn't
 ever run and store this per block.
 
\end_layout

\begin_layout Standard
Write special code without these fragments and optimise aggressively.
 ???????
\end_layout

\begin_layout Standard
Use pragmas to speed this up
\end_layout

\begin_layout Standard
Validate vectorisation has been successful (VTune)
\end_layout

\begin_layout Standard

\series bold
maybe calculate speed-up on actual number of processors used.
 
\end_layout

\begin_layout Section*
Step 3
\end_layout

\begin_layout Standard
Parrallelise step 2 on multicore node (further instructions on coursework
 spec)
\end_layout

\begin_layout Standard
Setup scenario function will be important here
\end_layout

\begin_layout Section*
Notes from chat
\end_layout

\begin_layout Standard
Have a look at the profiling lecture/practical
\end_layout

\begin_layout Standard
He means stuff like hot functions
\end_layout

\begin_layout Standard
Step 1 - Profile it before you change anything to make sure it's getting
 better
\end_layout

\begin_layout Standard
Vectorisation is SIMD, parallelization is MIMD
\end_layout

\begin_layout Standard
You parallelize outer loops and vectorize inner loops
\end_layout

\begin_layout Standard
Don't forget to compile with -g3 --> Keeps source information in the binary,
 that way you can see line by line the expensive stuff
\end_layout

\begin_layout Standard
Can confirm the parameters matter a lot First one: size of the box Or length
 more accurately I think Second one is when to plot a file (use 0 to disable)
 Last one is density of fluid: haven't really figured out the effect of
 this one yet
\end_layout

\begin_layout Standard
Need to run for a range of parameters and get an average?
\end_layout

\begin_layout Standard
Ben do you think the model we should use should be weak scaling? --> Need
 to have a look over those lecture notes but theory yes as I can't see it
 being memory bound but could be wrong
\end_layout

\begin_layout Standard
Memory bound --> Doesn't matter if your CPU can do 1 GFLOPS if you can only
 do 0.5GFLOpS because data can't be accessed fast enough So parallelising
 the code won't help But vectorising would assuming you haven't saturated
 the memory link --> the roofline model 
\end_layout

\begin_layout Standard
It's all those graphs Tobias drew in the lectures --> The diagonal is the
 compute (performance) bound right? --> Yep --> And the horizontal is when
 the memory can't provide data fast enough --> Yep
\end_layout

\begin_layout Standard
we need to calculate what the operation:byte ratio is of the expensive code
\end_layout

\begin_layout Standard
Probably only need hamilton for timing, if at all
\end_layout

\begin_layout Standard
Ben's Tobias Email:
\end_layout

\begin_layout Standard
> The majority of time spent (~85%) is in the computeP() which only has
 a single if statement and as 98% of the cells are inside the domain I don't
 see how optimising blocks that are outside of the domain would result in
 a speed up.
 
\end_layout

\begin_layout Standard
Why outside of the domain? It might be interesting to see whether a block
 holds any flagged cells.
 The ifs are not really an issue in a standard code.
 But what happens if you wanna vectorise?
\end_layout

\begin_layout Standard
Tobias
\end_layout

\begin_layout Standard
I'm assuming a high arithmetic intensity implies compute bound --> yes
\end_layout

\begin_layout Standard
Basically if a section is slow but it doesn't do many operations per loaded
 byte It's likely memory bound
\end_layout

\begin_layout Standard
PRACTICALS HAVE STUFF ON ARITHMETIC INTENSITY CALCULATIONS
\end_layout

\begin_layout Standard
(Gustafson vs Amdahl) No but he does give you the properties of each, To
 match with the properties of our code, Amdahl's law is probably the one
 as its pretty popular
\end_layout

\begin_layout Standard

\series bold
BEN --> Im going with strong scaling/amdahls law because we only know the
 time for a single processor anyway and its easier to talk about
\end_layout

\end_body
\end_document
